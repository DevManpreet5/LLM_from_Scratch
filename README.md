# LLM_from_Scratch ðŸš€

This repository documents my journey of building a **Large Language Model (LLM) from scratch**

## Progress Log

### Day 1: Understanding LLMs & Revisiting Fundamentals

- Studied the **basics of Large Language Models (LLMs)**
- Revised **Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM)** networks
- Started reading:
  - ðŸ“„ [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
  - ðŸ“„ [Language Models Are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

### Day 2: Tokenization & Preprocessing

- Implemented a **simple tokenizer** from scratch â†’ [Tokenizer.ipynb](1_Tokenizer/Tokenizer.ipynb)
- Added **special character tokens**
- Implemented **Byte Pair Encoding (BPE)** using `tiktoken` â†’ [Bytepairencoding.ipynb](1_Tokenizer/Bytepairencoding.ipynb)
- Used sample text data â†’ [verdictbook.txt](1_Tokenizer/verdictbook.txt)
