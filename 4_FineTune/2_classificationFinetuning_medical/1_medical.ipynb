{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    \n",
    "    \"context_length\": 1024, \n",
    "    \"emb_dim\": 768,         \n",
    "    \"n_heads\": 12,          \n",
    "    \"n_layers\": 12,         \n",
    "    \"drop_rate\": 0.1,       \n",
    "    \"qkv_bias\": True       \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiheadv2(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,attention_head,boolbias):\n",
    "        super().__init__()\n",
    "        self.head_dim=d_out//attention_head\n",
    "        self.d_out=d_out\n",
    "        self.attention_head=attention_head\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=boolbias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=boolbias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=boolbias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        b,num_token,d_out=x.shape\n",
    "\n",
    "        keys = self.W_key(x) \n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys=keys.view(b,num_token,self.attention_head,self.head_dim)\n",
    "        queries=queries.view(b,num_token,self.attention_head,self.head_dim)\n",
    "        values=values.view(b,num_token,self.attention_head,self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_score=queries @ keys.transpose(2,3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_token, :num_token]\n",
    "        attn_score.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_score / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        context_vec = context_vec.contiguous().view(b, num_token, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale_params=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift_params=nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return norm*self.scale_params+ self.shift_params\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) *(x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "    \n",
    "\n",
    "class feedforward(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(config['emb_dim'],config['emb_dim']*4),\n",
    "            GELU(),\n",
    "            nn.Linear(config['emb_dim']*4,config['emb_dim']),\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__ (self,config):\n",
    "        super().__init__()\n",
    "        self.attn=multiheadv2(d_in=config['emb_dim'],d_out=config['emb_dim'],context_length=config['context_length'],dropout=config['drop_rate'],attention_head=config['n_heads'],boolbias=config['qkv_bias'])\n",
    "        self.Layernorm1=LayerNorm(emb_dim=config['emb_dim'])\n",
    "        self.Layernorm2=LayerNorm(emb_dim=config['emb_dim'])\n",
    "        self.feedforw=feedforward(config=config)\n",
    "        self.dropout=nn.Dropout(config['drop_rate'])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        ## attnetion block\n",
    "        skip=x\n",
    "        x=self.Layernorm1(x)\n",
    "        x=self.attn(x)\n",
    "        x=self.dropout(x)\n",
    "        x=x+skip\n",
    "\n",
    "        ## feed forward nn block\n",
    "        skip=x\n",
    "        x=self.Layernorm2(x)\n",
    "        x=self.feedforw(x)\n",
    "        x=self.dropout(x)\n",
    "        x=x+skip\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_2(nn.Module):\n",
    "    def __init__ (self,cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb=nn.Embedding(cfg['vocab_size'],cfg[\"emb_dim\"])\n",
    "        self.pos_emb=nn.Embedding(cfg['context_length'],cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self,inputidx):\n",
    "        batch_size,seq=inputidx.shape\n",
    "        tokens=self.token_emb(inputidx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq, device=inputidx.device))\n",
    "        x=tokens+pos_embeds\n",
    "        x=self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPEN AI weights are in tensorflow format , we need to convert them to torch compatible first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptweightdownload import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.token_emb.weight = assign(gpt.token_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].attn.W_query.weight = assign(gpt.trf_blocks[b].attn.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].attn.W_key.weight = assign(gpt.trf_blocks[b].attn.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].attn.W_value.weight = assign(gpt.trf_blocks[b].attn.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].attn.W_query.bias = assign(gpt.trf_blocks[b].attn.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].attn.W_key.bias = assign(gpt.trf_blocks[b].attn.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].attn.W_value.bias = assign(gpt.trf_blocks[b].attn.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].attn.out_proj.weight = assign(gpt.trf_blocks[b].attn.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].attn.out_proj.bias = assign(gpt.trf_blocks[b].attn.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].feedforw.layers[0].weight = assign(gpt.trf_blocks[b].feedforw.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].feedforw.layers[0].bias = assign(gpt.trf_blocks[b].feedforw.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].feedforw.layers[2].weight = assign(gpt.trf_blocks[b].feedforw.layers[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].feedforw.layers[2].bias = assign(gpt.trf_blocks[b].feedforw.layers[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].Layernorm1.scale_params = assign(gpt.trf_blocks[b].Layernorm1.scale_params, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].Layernorm1.shift_params = assign(gpt.trf_blocks[b].Layernorm1.shift_params, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].Layernorm2.scale_params = assign(gpt.trf_blocks[b].Layernorm2.scale_params, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].Layernorm2.shift_params = assign(gpt.trf_blocks[b].Layernorm2.shift_params, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale_params = assign(gpt.final_norm.scale_params, params[\"g\"])\n",
    "    gpt.final_norm.shift_params = assign(gpt.final_norm.shift_params, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_token_size, context_Size, top_k=5, temperature=1.0):\n",
    "    for _ in range(max_token_size):\n",
    "        idx_split = idx[:, -context_Size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_split)\n",
    "\n",
    "        logits_last = logits[:, -1, :]\n",
    "        \n",
    "        logits_last = logits_last / temperature\n",
    "        \n",
    "       \n",
    "        values, indices = torch.topk(logits_last, k=top_k, dim=-1)\n",
    "        probs = torch.softmax(values, dim=-1)\n",
    "        sampled_idx = torch.multinomial(probs, num_samples=1)\n",
    "        maxi = torch.gather(indices, -1, sampled_idx)\n",
    "       \n",
    "    \n",
    "        idx = torch.cat((idx, maxi), dim=1)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) \n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) \n",
    "    return tokenizer.decode(flat.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT_2(\n",
       "  (token_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): multiheadv2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (Layernorm1): LayerNorm()\n",
       "      (Layernorm2): LayerNorm()\n",
       "      (feedforw): feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): multiheadv2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (Layernorm1): LayerNorm()\n",
       "      (Layernorm2): LayerNorm()\n",
       "      (feedforw): feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): multiheadv2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (Layernorm1): LayerNorm()\n",
       "      (Layernorm2): LayerNorm()\n",
       "      (feedforw): feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): multiheadv2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (Layernorm1): LayerNorm()\n",
       "      (Layernorm2): LayerNorm()\n",
       "      (feedforw): feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): multiheadv2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (Layernorm1): LayerNorm()\n",
       "      (Layernorm2): LayerNorm()\n",
       "      (feedforw): feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): multiheadv2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (Layernorm1): LayerNorm()\n",
       "      (Layernorm2): LayerNorm()\n",
       "      (feedforw): feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): multiheadv2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (Layernorm1): LayerNorm()\n",
       "      (Layernorm2): LayerNorm()\n",
       "      (feedforw): feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): multiheadv2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (Layernorm1): LayerNorm()\n",
       "      (Layernorm2): LayerNorm()\n",
       "      (feedforw): feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): multiheadv2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (Layernorm1): LayerNorm()\n",
       "      (Layernorm2): LayerNorm()\n",
       "      (feedforw): feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): multiheadv2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (Layernorm1): LayerNorm()\n",
       "      (Layernorm2): LayerNorm()\n",
       "      (feedforw): feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): multiheadv2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (Layernorm1): LayerNorm()\n",
       "      (Layernorm2): LayerNorm()\n",
       "      (feedforw): feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): multiheadv2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (Layernorm1): LayerNorm()\n",
       "      (Layernorm2): LayerNorm()\n",
       "      (feedforw): feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT_2(GPT_CONFIG_124M)\n",
    "load_weights_into_gpt(model, params)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## after weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weight checksum: -47459.91598956287\n"
     ]
    }
   ],
   "source": [
    "def compute_checksum(model):\n",
    "    return sum(p.sum().item() for p in model.parameters())\n",
    "\n",
    "checksum = compute_checksum(model)\n",
    "print(f\"Model weight checksum: {checksum}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This study analyzed liver function abnormaliti...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A post hoc analysis was conducted with the use...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Liver function tests ( LFTs ) were measured at...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Survival analyses were used to assess the asso...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The percentage of patients with abnormal LFTs ...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30130</th>\n",
       "      <td>There was a statistically significant between-...</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30131</th>\n",
       "      <td>There were no statistically significant betwee...</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30132</th>\n",
       "      <td>There was no significant association between s...</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30133</th>\n",
       "      <td>No adverse effects were reported .</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30134</th>\n",
       "      <td>Performing a 6-week do-as-tolerated program of...</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30135 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           abstract_text  line_number  \\\n",
       "0      This study analyzed liver function abnormaliti...            0   \n",
       "1      A post hoc analysis was conducted with the use...            1   \n",
       "2      Liver function tests ( LFTs ) were measured at...            2   \n",
       "3      Survival analyses were used to assess the asso...            3   \n",
       "4      The percentage of patients with abnormal LFTs ...            4   \n",
       "...                                                  ...          ...   \n",
       "30130  There was a statistically significant between-...           13   \n",
       "30131  There were no statistically significant betwee...           14   \n",
       "30132  There was no significant association between s...           15   \n",
       "30133                 No adverse effects were reported .           16   \n",
       "30134  Performing a 6-week do-as-tolerated program of...           17   \n",
       "\n",
       "       total_lines  target  \n",
       "0                9       0  \n",
       "1                9       4  \n",
       "2                9       4  \n",
       "3                9       4  \n",
       "4                9       4  \n",
       "...            ...     ...  \n",
       "30130           18       4  \n",
       "30131           18       4  \n",
       "30132           18       4  \n",
       "30133           18       4  \n",
       "30134           18       1  \n",
       "\n",
       "[30135 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_train = df_train[['abstract_text', 'line_number', 'total_lines', 'target']]\n",
    "df_train['target'] = df_train['target'].astype('category').cat.codes\n",
    "df_train\n",
    "\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test = df_test[['abstract_text', 'line_number', 'total_lines', 'target']]\n",
    "df_test['target'] = df_test['target'].astype('category').cat.codes\n",
    "df_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train.csv\", index=None)\n",
    "df_test.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=128, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file).dropna()  \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length if max_length is not None else 128  \n",
    "\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text)[:self.max_length] for text in self.data[\"abstract_text\"]\n",
    "        ]\n",
    "\n",
    "        self.encoded_texts = [\n",
    "            encoded + [pad_token_id] * (self.max_length - len(encoded)) for encoded in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "        self.labels = torch.tensor(self.data[\"target\"].astype('category').cat.codes, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.encoded_texts[idx], dtype=torch.long), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "train_dataset = MedicalDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)\n",
    "\n",
    "\n",
    "val_dataset = MedicalDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = MedicalDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(test_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 5\n",
    "model.out_head = torch.nn.Linear(in_features=GPT_CONFIG_124M[\"emb_dim\"], out_features=num_classes)\n",
    "\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]  \n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]  \n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() \n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() \n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            examples_seen += input_batch.shape[0] \n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.990, Val loss 3.314\n",
      "Ep 1 (Step 000050): Train loss 1.573, Val loss 1.427\n",
      "Ep 1 (Step 000100): Train loss 1.659, Val loss 1.437\n",
      "Ep 1 (Step 000150): Train loss 1.593, Val loss 1.399\n",
      "Ep 1 (Step 000200): Train loss 1.421, Val loss 1.321\n",
      "Ep 1 (Step 000250): Train loss 1.650, Val loss 1.345\n",
      "Ep 1 (Step 000300): Train loss 1.482, Val loss 1.387\n",
      "Ep 1 (Step 000350): Train loss 1.499, Val loss 1.395\n",
      "Ep 1 (Step 000400): Train loss 1.499, Val loss 1.366\n",
      "Ep 1 (Step 000450): Train loss 1.439, Val loss 1.329\n",
      "Ep 1 (Step 000500): Train loss 1.479, Val loss 1.363\n",
      "Ep 1 (Step 000550): Train loss 1.440, Val loss 1.309\n",
      "Ep 1 (Step 000600): Train loss 1.531, Val loss 1.321\n",
      "Ep 1 (Step 000650): Train loss 1.450, Val loss 1.320\n",
      "Ep 1 (Step 000700): Train loss 1.225, Val loss 1.279\n",
      "Ep 1 (Step 000750): Train loss 1.392, Val loss 1.265\n",
      "Ep 1 (Step 000800): Train loss 1.195, Val loss 1.238\n",
      "Ep 1 (Step 000850): Train loss 1.217, Val loss 1.270\n",
      "Ep 1 (Step 000900): Train loss 1.228, Val loss 1.220\n",
      "Ep 1 (Step 000950): Train loss 1.113, Val loss 1.147\n",
      "Ep 1 (Step 001000): Train loss 1.217, Val loss 1.081\n",
      "Ep 1 (Step 001050): Train loss 1.080, Val loss 1.074\n",
      "Ep 1 (Step 001100): Train loss 1.092, Val loss 1.072\n",
      "Ep 1 (Step 001150): Train loss 1.125, Val loss 1.038\n",
      "Ep 1 (Step 001200): Train loss 0.971, Val loss 1.129\n",
      "Ep 1 (Step 001250): Train loss 1.109, Val loss 0.959\n",
      "Ep 1 (Step 001300): Train loss 1.105, Val loss 1.087\n",
      "Ep 1 (Step 001350): Train loss 0.982, Val loss 0.993\n",
      "Ep 1 (Step 001400): Train loss 0.938, Val loss 1.062\n",
      "Ep 1 (Step 001450): Train loss 1.149, Val loss 1.091\n",
      "Ep 1 (Step 001500): Train loss 0.845, Val loss 1.063\n",
      "Ep 1 (Step 001550): Train loss 1.017, Val loss 1.018\n",
      "Ep 1 (Step 001600): Train loss 0.888, Val loss 1.027\n",
      "Ep 1 (Step 001650): Train loss 0.723, Val loss 1.075\n",
      "Ep 1 (Step 001700): Train loss 1.122, Val loss 1.028\n",
      "Ep 1 (Step 001750): Train loss 0.958, Val loss 0.932\n",
      "Ep 1 (Step 001800): Train loss 1.026, Val loss 1.073\n",
      "Ep 1 (Step 001850): Train loss 1.024, Val loss 1.015\n",
      "Ep 1 (Step 001900): Train loss 0.828, Val loss 1.014\n",
      "Ep 1 (Step 001950): Train loss 0.839, Val loss 0.899\n",
      "Ep 1 (Step 002000): Train loss 0.844, Val loss 0.961\n",
      "Ep 1 (Step 002050): Train loss 1.094, Val loss 0.989\n",
      "Ep 1 (Step 002100): Train loss 0.677, Val loss 1.016\n",
      "Ep 1 (Step 002150): Train loss 0.790, Val loss 1.003\n",
      "Ep 1 (Step 002200): Train loss 0.758, Val loss 1.009\n",
      "Ep 1 (Step 002250): Train loss 0.792, Val loss 0.992\n",
      "Ep 1 (Step 002300): Train loss 0.824, Val loss 1.075\n",
      "Ep 1 (Step 002350): Train loss 0.958, Val loss 1.123\n",
      "Ep 1 (Step 002400): Train loss 0.775, Val loss 1.125\n",
      "Ep 1 (Step 002450): Train loss 0.470, Val loss 0.946\n",
      "Ep 1 (Step 002500): Train loss 0.653, Val loss 1.036\n",
      "Ep 1 (Step 002550): Train loss 0.787, Val loss 0.919\n",
      "Ep 1 (Step 002600): Train loss 0.836, Val loss 1.064\n",
      "Ep 1 (Step 002650): Train loss 0.672, Val loss 1.157\n",
      "Ep 1 (Step 002700): Train loss 0.925, Val loss 1.036\n",
      "Ep 1 (Step 002750): Train loss 0.782, Val loss 1.144\n",
      "Ep 1 (Step 002800): Train loss 0.948, Val loss 1.119\n",
      "Ep 1 (Step 002850): Train loss 0.782, Val loss 1.147\n",
      "Ep 1 (Step 002900): Train loss 0.786, Val loss 1.124\n",
      "Ep 1 (Step 002950): Train loss 0.803, Val loss 0.979\n",
      "Ep 1 (Step 003000): Train loss 0.602, Val loss 1.006\n",
      "Ep 1 (Step 003050): Train loss 0.884, Val loss 1.042\n",
      "Ep 1 (Step 003100): Train loss 0.829, Val loss 1.021\n",
      "Ep 1 (Step 003150): Train loss 0.555, Val loss 1.033\n",
      "Ep 1 (Step 003200): Train loss 0.686, Val loss 1.013\n",
      "Ep 1 (Step 003250): Train loss 0.703, Val loss 1.057\n",
      "Ep 1 (Step 003300): Train loss 0.650, Val loss 1.050\n",
      "Ep 1 (Step 003350): Train loss 0.565, Val loss 1.063\n",
      "Ep 1 (Step 003400): Train loss 0.507, Val loss 1.063\n",
      "Ep 1 (Step 003450): Train loss 0.585, Val loss 1.191\n",
      "Ep 1 (Step 003500): Train loss 0.527, Val loss 1.123\n",
      "Ep 1 (Step 003550): Train loss 0.833, Val loss 1.107\n",
      "Ep 1 (Step 003600): Train loss 0.919, Val loss 1.044\n",
      "Ep 1 (Step 003650): Train loss 0.722, Val loss 1.003\n",
      "Ep 1 (Step 003700): Train loss 0.688, Val loss 1.117\n",
      "Ep 1 (Step 003750): Train loss 0.428, Val loss 1.028\n",
      "Ep 1 (Step 003800): Train loss 0.995, Val loss 1.094\n",
      "Ep 1 (Step 003850): Train loss 0.652, Val loss 1.051\n",
      "Ep 1 (Step 003900): Train loss 0.666, Val loss 1.027\n",
      "Ep 1 (Step 003950): Train loss 0.624, Val loss 1.059\n",
      "Ep 1 (Step 004000): Train loss 0.973, Val loss 1.142\n",
      "Ep 1 (Step 004050): Train loss 0.765, Val loss 1.005\n",
      "Ep 1 (Step 004100): Train loss 0.624, Val loss 1.088\n",
      "Ep 1 (Step 004150): Train loss 0.745, Val loss 1.149\n",
      "Ep 1 (Step 004200): Train loss 1.007, Val loss 1.155\n",
      "Ep 1 (Step 004250): Train loss 0.621, Val loss 1.030\n",
      "Ep 1 (Step 004300): Train loss 0.899, Val loss 1.126\n",
      "Ep 1 (Step 004350): Train loss 0.650, Val loss 1.093\n",
      "Ep 1 (Step 004400): Train loss 0.701, Val loss 1.176\n",
      "Ep 1 (Step 004450): Train loss 0.784, Val loss 1.059\n",
      "Ep 1 (Step 004500): Train loss 0.628, Val loss 1.065\n",
      "Ep 1 (Step 004550): Train loss 0.890, Val loss 1.161\n",
      "Ep 1 (Step 004600): Train loss 0.754, Val loss 1.010\n",
      "Ep 1 (Step 004650): Train loss 0.891, Val loss 0.985\n",
      "Ep 1 (Step 004700): Train loss 0.722, Val loss 1.055\n",
      "Ep 1 (Step 004750): Train loss 0.639, Val loss 0.959\n",
      "Ep 1 (Step 004800): Train loss 0.718, Val loss 0.994\n",
      "Ep 1 (Step 004850): Train loss 0.769, Val loss 1.182\n",
      "Ep 1 (Step 004900): Train loss 0.687, Val loss 1.166\n",
      "Ep 1 (Step 004950): Train loss 0.897, Val loss 1.282\n",
      "Ep 1 (Step 005000): Train loss 0.838, Val loss 1.083\n",
      "Ep 1 (Step 005050): Train loss 1.031, Val loss 1.185\n",
      "Ep 1 (Step 005100): Train loss 0.545, Val loss 1.008\n",
      "Ep 1 (Step 005150): Train loss 0.580, Val loss 1.160\n",
      "Ep 1 (Step 005200): Train loss 0.676, Val loss 1.062\n",
      "Ep 1 (Step 005250): Train loss 0.621, Val loss 1.160\n",
      "Ep 1 (Step 005300): Train loss 0.439, Val loss 1.138\n",
      "Ep 1 (Step 005350): Train loss 0.750, Val loss 1.044\n",
      "Ep 1 (Step 005400): Train loss 0.613, Val loss 1.043\n",
      "Ep 1 (Step 005450): Train loss 0.448, Val loss 1.111\n",
      "Ep 1 (Step 005500): Train loss 0.856, Val loss 0.996\n",
      "Ep 1 (Step 005550): Train loss 0.513, Val loss 1.020\n",
      "Ep 1 (Step 005600): Train loss 0.876, Val loss 1.173\n",
      "Ep 1 (Step 005650): Train loss 0.737, Val loss 1.115\n",
      "Ep 1 (Step 005700): Train loss 0.972, Val loss 1.049\n",
      "Ep 1 (Step 005750): Train loss 0.847, Val loss 1.122\n",
      "Ep 1 (Step 005800): Train loss 0.621, Val loss 1.172\n",
      "Ep 1 (Step 005850): Train loss 0.398, Val loss 1.106\n",
      "Ep 1 (Step 005900): Train loss 0.864, Val loss 1.162\n",
      "Ep 1 (Step 005950): Train loss 0.593, Val loss 1.041\n",
      "Ep 1 (Step 006000): Train loss 0.632, Val loss 0.895\n",
      "Ep 1 (Step 006050): Train loss 0.810, Val loss 1.016\n",
      "Ep 1 (Step 006100): Train loss 1.006, Val loss 1.146\n",
      "Ep 1 (Step 006150): Train loss 0.749, Val loss 1.266\n",
      "Ep 1 (Step 006200): Train loss 0.881, Val loss 0.950\n",
      "Ep 1 (Step 006250): Train loss 0.783, Val loss 0.994\n",
      "Ep 1 (Step 006300): Train loss 0.678, Val loss 0.964\n",
      "Ep 1 (Step 006350): Train loss 0.577, Val loss 1.034\n",
      "Ep 1 (Step 006400): Train loss 0.796, Val loss 1.044\n",
      "Ep 1 (Step 006450): Train loss 0.496, Val loss 1.063\n",
      "Ep 1 (Step 006500): Train loss 0.344, Val loss 1.069\n",
      "Ep 1 (Step 006550): Train loss 0.427, Val loss 1.010\n",
      "Ep 1 (Step 006600): Train loss 0.807, Val loss 1.076\n",
      "Ep 1 (Step 006650): Train loss 0.496, Val loss 1.088\n",
      "Ep 1 (Step 006700): Train loss 0.645, Val loss 1.012\n",
      "Ep 1 (Step 006750): Train loss 0.641, Val loss 1.106\n",
      "Ep 1 (Step 006800): Train loss 0.529, Val loss 1.077\n",
      "Ep 1 (Step 006850): Train loss 0.803, Val loss 0.943\n",
      "Ep 1 (Step 006900): Train loss 0.542, Val loss 1.104\n",
      "Ep 1 (Step 006950): Train loss 0.381, Val loss 0.997\n",
      "Ep 1 (Step 007000): Train loss 0.465, Val loss 1.103\n",
      "Ep 1 (Step 007050): Train loss 0.668, Val loss 0.991\n",
      "Ep 1 (Step 007100): Train loss 0.592, Val loss 1.104\n",
      "Ep 1 (Step 007150): Train loss 0.656, Val loss 0.949\n",
      "Ep 1 (Step 007200): Train loss 0.446, Val loss 1.012\n",
      "Ep 1 (Step 007250): Train loss 0.719, Val loss 1.059\n",
      "Ep 1 (Step 007300): Train loss 0.693, Val loss 1.079\n",
      "Ep 1 (Step 007350): Train loss 0.577, Val loss 0.935\n",
      "Ep 1 (Step 007400): Train loss 0.505, Val loss 0.982\n",
      "Ep 1 (Step 007450): Train loss 0.555, Val loss 1.110\n",
      "Ep 1 (Step 007500): Train loss 0.657, Val loss 0.977\n",
      "Ep 1 (Step 007550): Train loss 0.438, Val loss 0.866\n",
      "Ep 1 (Step 007600): Train loss 0.649, Val loss 0.917\n",
      "Ep 1 (Step 007650): Train loss 0.612, Val loss 0.937\n",
      "Ep 1 (Step 007700): Train loss 0.514, Val loss 1.102\n",
      "Ep 1 (Step 007750): Train loss 0.534, Val loss 0.869\n",
      "Ep 1 (Step 007800): Train loss 0.781, Val loss 0.964\n",
      "Ep 1 (Step 007850): Train loss 0.529, Val loss 1.075\n",
      "Ep 1 (Step 007900): Train loss 0.832, Val loss 0.992\n",
      "Ep 1 (Step 007950): Train loss 0.573, Val loss 1.049\n",
      "Ep 1 (Step 008000): Train loss 0.851, Val loss 0.991\n",
      "Ep 1 (Step 008050): Train loss 0.733, Val loss 1.018\n",
      "Ep 1 (Step 008100): Train loss 0.518, Val loss 0.902\n",
      "Ep 1 (Step 008150): Train loss 0.801, Val loss 0.961\n",
      "Ep 1 (Step 008200): Train loss 0.333, Val loss 0.862\n",
      "Ep 1 (Step 008250): Train loss 0.631, Val loss 0.965\n",
      "Ep 1 (Step 008300): Train loss 0.401, Val loss 0.942\n",
      "Ep 1 (Step 008350): Train loss 0.772, Val loss 1.026\n",
      "Ep 1 (Step 008400): Train loss 0.374, Val loss 0.862\n",
      "Ep 1 (Step 008450): Train loss 0.273, Val loss 0.871\n",
      "Ep 1 (Step 008500): Train loss 0.634, Val loss 0.913\n",
      "Ep 1 (Step 008550): Train loss 0.514, Val loss 0.988\n",
      "Ep 1 (Step 008600): Train loss 0.595, Val loss 0.986\n",
      "Ep 1 (Step 008650): Train loss 0.551, Val loss 1.034\n",
      "Ep 1 (Step 008700): Train loss 0.381, Val loss 1.046\n",
      "Ep 1 (Step 008750): Train loss 0.597, Val loss 0.934\n",
      "Ep 1 (Step 008800): Train loss 0.500, Val loss 1.005\n",
      "Ep 1 (Step 008850): Train loss 0.454, Val loss 1.148\n",
      "Ep 1 (Step 008900): Train loss 1.025, Val loss 0.960\n",
      "Ep 1 (Step 008950): Train loss 0.398, Val loss 0.948\n",
      "Ep 1 (Step 009000): Train loss 0.454, Val loss 0.985\n",
      "Ep 1 (Step 009050): Train loss 0.537, Val loss 0.905\n",
      "Ep 1 (Step 009100): Train loss 0.440, Val loss 1.156\n",
      "Ep 1 (Step 009150): Train loss 0.682, Val loss 1.026\n",
      "Ep 1 (Step 009200): Train loss 0.602, Val loss 1.180\n",
      "Ep 1 (Step 009250): Train loss 0.486, Val loss 1.078\n",
      "Ep 1 (Step 009300): Train loss 0.592, Val loss 0.983\n",
      "Ep 1 (Step 009350): Train loss 0.522, Val loss 1.086\n",
      "Ep 1 (Step 009400): Train loss 0.753, Val loss 1.058\n",
      "Ep 1 (Step 009450): Train loss 0.691, Val loss 0.911\n",
      "Ep 1 (Step 009500): Train loss 0.293, Val loss 0.843\n",
      "Ep 1 (Step 009550): Train loss 0.648, Val loss 1.042\n",
      "Ep 1 (Step 009600): Train loss 0.408, Val loss 1.010\n",
      "Ep 1 (Step 009650): Train loss 0.435, Val loss 1.149\n",
      "Ep 1 (Step 009700): Train loss 0.291, Val loss 0.960\n",
      "Ep 1 (Step 009750): Train loss 0.414, Val loss 0.941\n",
      "Ep 1 (Step 009800): Train loss 0.473, Val loss 0.871\n",
      "Ep 1 (Step 009850): Train loss 0.499, Val loss 1.031\n",
      "Ep 1 (Step 009900): Train loss 0.443, Val loss 1.017\n",
      "Ep 1 (Step 009950): Train loss 0.538, Val loss 0.951\n",
      "Ep 1 (Step 010000): Train loss 0.563, Val loss 0.897\n",
      "Ep 1 (Step 010050): Train loss 0.530, Val loss 0.994\n",
      "Ep 1 (Step 010100): Train loss 0.640, Val loss 1.043\n",
      "Ep 1 (Step 010150): Train loss 0.715, Val loss 0.950\n",
      "Ep 1 (Step 010200): Train loss 0.539, Val loss 0.884\n",
      "Ep 1 (Step 010250): Train loss 0.744, Val loss 0.939\n",
      "Ep 1 (Step 010300): Train loss 0.697, Val loss 0.820\n",
      "Ep 1 (Step 010350): Train loss 0.457, Val loss 0.827\n",
      "Ep 1 (Step 010400): Train loss 0.645, Val loss 0.960\n",
      "Ep 1 (Step 010450): Train loss 0.464, Val loss 0.996\n",
      "Ep 1 (Step 010500): Train loss 0.388, Val loss 1.129\n",
      "Ep 1 (Step 010550): Train loss 0.447, Val loss 0.863\n",
      "Ep 1 (Step 010600): Train loss 0.642, Val loss 1.097\n",
      "Ep 1 (Step 010650): Train loss 0.581, Val loss 1.031\n",
      "Ep 1 (Step 010700): Train loss 0.828, Val loss 0.964\n",
      "Ep 1 (Step 010750): Train loss 0.800, Val loss 0.915\n",
      "Ep 1 (Step 010800): Train loss 0.288, Val loss 0.814\n",
      "Ep 1 (Step 010850): Train loss 0.427, Val loss 0.982\n",
      "Ep 1 (Step 010900): Train loss 0.687, Val loss 1.023\n",
      "Ep 1 (Step 010950): Train loss 0.568, Val loss 0.889\n",
      "Ep 1 (Step 011000): Train loss 0.360, Val loss 1.079\n",
      "Ep 1 (Step 011050): Train loss 0.537, Val loss 0.909\n",
      "Ep 1 (Step 011100): Train loss 0.504, Val loss 1.019\n",
      "Ep 1 (Step 011150): Train loss 0.547, Val loss 1.088\n",
      "Ep 1 (Step 011200): Train loss 0.413, Val loss 1.005\n",
      "Ep 1 (Step 011250): Train loss 0.422, Val loss 0.995\n",
      "Ep 1 (Step 011300): Train loss 0.478, Val loss 0.837\n",
      "Ep 1 (Step 011350): Train loss 0.431, Val loss 0.900\n",
      "Ep 1 (Step 011400): Train loss 0.582, Val loss 0.970\n",
      "Ep 1 (Step 011450): Train loss 0.733, Val loss 1.081\n",
      "Ep 1 (Step 011500): Train loss 0.844, Val loss 1.010\n",
      "Ep 1 (Step 011550): Train loss 0.692, Val loss 0.927\n",
      "Ep 1 (Step 011600): Train loss 0.423, Val loss 1.141\n",
      "Ep 1 (Step 011650): Train loss 0.395, Val loss 0.992\n",
      "Ep 1 (Step 011700): Train loss 0.438, Val loss 0.849\n",
      "Ep 1 (Step 011750): Train loss 0.514, Val loss 1.012\n",
      "Ep 1 (Step 011800): Train loss 0.521, Val loss 0.863\n",
      "Ep 1 (Step 011850): Train loss 0.476, Val loss 0.837\n",
      "Ep 1 (Step 011900): Train loss 0.696, Val loss 1.054\n",
      "Ep 1 (Step 011950): Train loss 0.816, Val loss 0.868\n",
      "Ep 1 (Step 012000): Train loss 0.247, Val loss 0.955\n",
      "Ep 1 (Step 012050): Train loss 0.530, Val loss 0.850\n",
      "Ep 1 (Step 012100): Train loss 0.522, Val loss 0.925\n",
      "Ep 1 (Step 012150): Train loss 0.444, Val loss 0.997\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m     10\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 11\u001b[0m train_losses, val_losses, train_accs, val_accs, examples_seen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_classifier_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     17\u001b[0m execution_time_minutes \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
      "Cell \u001b[0;32mIn[24], line 11\u001b[0m, in \u001b[0;36mtrain_classifier_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m---> 11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward() \n\u001b[1;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep() \n",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m, in \u001b[0;36mcalc_loss_batch\u001b[0;34m(input_batch, target_batch, model, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalc_loss_batch\u001b[39m(input_batch, target_batch, model, device):\n\u001b[0;32m----> 2\u001b[0m     input_batch, target_batch \u001b[38;5;241m=\u001b[39m \u001b[43minput_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(input_batch)[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(logits, target_batch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "model.to(device) \n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train_accuracy = calc_accuracy_loader(train_loader, model, device)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# val_accuracy = calc_accuracy_loader(val_loader, model, device)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_accuracy_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m, in \u001b[0;36mcalc_accuracy_loader\u001b[0;34m(data_loader, model, device, num_batches)\u001b[0m\n\u001b[1;32m     11\u001b[0m input_batch, target_batch \u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mto(device), target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 14\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \n\u001b[1;32m     15\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m num_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m predicted_labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36mGPT_2.forward\u001b[0;34m(self, inputidx)\u001b[0m\n\u001b[1;32m     19\u001b[0m x\u001b[38;5;241m=\u001b[39mtokens\u001b[38;5;241m+\u001b[39mpos_embeds\n\u001b[1;32m     20\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_emb(x)\n\u001b[0;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrf_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(x)\n\u001b[1;32m     23\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_head(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m## feed forward nn block\u001b[39;00m\n\u001b[1;32m     19\u001b[0m skip\u001b[38;5;241m=\u001b[39mx\n\u001b[0;32m---> 20\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayernorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeedforw(x)\n\u001b[1;32m     22\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m mean\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m var\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mvar(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,unbiased\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m norm\u001b[38;5;241m=\u001b[39m(x\u001b[38;5;241m-\u001b[39mmean)\u001b[38;5;241m/\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m norm\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_params\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift_params\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJECTIVE\n",
      "{0: 'BACKGROUND', 1: 'CONCLUSIONS', 2: 'METHODS', 3: 'OBJECTIVE', 4: 'RESULTS'}\n"
     ]
    }
   ],
   "source": [
    "df_temp=pd.read_csv('train_2.csv')\n",
    "label_mapping = dict(enumerate(df_temp[\"target\"].astype(\"category\").cat.categories))\n",
    "\n",
    "def classify_review(text, model, tokenizer, device, label_mapping, max_length=128, pad_token_id=50256):\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(text)[:max_length]\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]  \n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return label_mapping[predicted_label]  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJECTIVE\n",
      "RESULTS\n",
      "METHODS\n",
      "BACKGROUND\n",
      "CONCLUSIONS\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"This study evaluates the effectiveness of a new treatment for knee arthritis.\"\n",
    "print(classify_review(text_1, model, tokenizer, device, label_mapping, max_length=train_dataset.max_length))\n",
    "\n",
    "text_2=\"Patients who received the new drug showed a 40% reduction in pain compared to the control group.\"\n",
    "print(classify_review(text_2, model, tokenizer, device, label_mapping, max_length=train_dataset.max_length))\n",
    "\n",
    "text_3=\"Participants were randomly assigned to either the treatment or placebo group and monitored for 12 weeks\"\n",
    "print(classify_review(text_3, model, tokenizer, device, label_mapping, max_length=train_dataset.max_length))\n",
    "\n",
    "text_4=\"Osteoarthritis is a leading cause of disability among older adults, affecting millions worldwide\"\n",
    "print(classify_review(text_4, model, tokenizer, device, label_mapping, max_length=train_dataset.max_length))\n",
    "\n",
    "text_5=\"Although the results were promising, the long-term effects of the treatment remain unknown and should be investigated further.\"\n",
    "print(classify_review(text_5, model, tokenizer, device, label_mapping, max_length=train_dataset.max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"biofinetunedEpoch5.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
